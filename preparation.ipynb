{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1b0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import env\n",
    "import numpy as np\n",
    "import acquire\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b1b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_zillow(df):\n",
    "    # drop redundant id code columns\n",
    "    id_cols = [col for col in df.columns if 'typeid' in col or col in ['id', 'parcelid']]\n",
    "    df = df.drop(columns=id_cols)\n",
    "    # filter for single family properties\n",
    "    df = df[df.propertylandusedesc == 'Single Family Residential']\n",
    "    # drop specified columns\n",
    "    cols_to_drop = ['calculatedbathnbr',\n",
    "                    'finishedfloor1squarefeet',\n",
    "                    'finishedsquarefeet12', \n",
    "                    'regionidcity',\n",
    "                    'landtaxvaluedollarcnt',\n",
    "                    'taxamount',\n",
    "                    'rawcensustractandblock',\n",
    "                    'roomcnt',\n",
    "                    'regionidcounty']\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    # fill null values with 0 in specified columns\n",
    "    cols_to_fill_zero = ['fireplacecnt',\n",
    "                         'garagecarcnt',\n",
    "                         'garagetotalsqft',\n",
    "                         'hashottuborspa',\n",
    "                         'poolcnt',\n",
    "                         'threequarterbathnbr',\n",
    "                         'taxdelinquencyflag']\n",
    "    for col in cols_to_fill_zero:\n",
    "        df[col] = np.where(df[col].isna(), 0, df[col]) \n",
    "    # drop columns with more than 5% null values\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().mean() > .05:\n",
    "            df = df.drop(columns=col)\n",
    "    # drop rows that remain with null values\n",
    "    df = df.dropna()   \n",
    "    # changing numeric codes to strings\n",
    "    df['fips'] = df.fips.apply(lambda fips: '0' + str(int(fips)))\n",
    "    df['regionidzip'] = df.regionidzip.apply(lambda x: str(int(x)))\n",
    "    df['censustractandblock'] = df.censustractandblock.apply(lambda x: str(int(x)))\n",
    "    # change the 'Y' in taxdelinquencyflag to 1\n",
    "    df['taxdelinquencyflag'] = np.where(df.taxdelinquencyflag == 'Y', 1, df.taxdelinquencyflag)\n",
    "    # change boolean column to int\n",
    "    df['hashottuborspa'] = df.hashottuborspa.apply(lambda x: str(int(x)))\n",
    "    # changing year from float to int\n",
    "    df['yearbuilt'] = df.yearbuilt.apply(lambda x: int(x))\n",
    "    df['assessmentyear'] = df.yearbuilt.apply(lambda x: int(x))\n",
    "    # adding a feature: age \n",
    "    df['age'] = 2017 - df.yearbuilt\n",
    "    # add a feature: has_garage\n",
    "    df['bool_has_garage'] = np.where(df.garagecarcnt > 0, 1, 0)\n",
    "    # add a feature: has_pool\n",
    "    df['bool_has_pool'] = np.where(df.poolcnt > 0, 1, 0)\n",
    "    # add a feature: has_fireplace\n",
    "    df['bool_has_fireplace'] = np.where(df.fireplacecnt > 0, 1, 0)\n",
    "    # adding prefix to boolean columns\n",
    "    df = df.rename(columns={'hashottuborspa': 'bool_hashottuborspa'})\n",
    "    df = df.rename(columns={'taxdelinquencyflag': 'bool_taxdelinquencyflag'})\n",
    "    # rename sqft column\n",
    "    df = df.rename(columns={'calculatedfinishedsquarefeet': 'sqft'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddf2ba6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vb/2f5pphz943qdprdg5zvhqsbm0000gp/T/ipykernel_4672/597513835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzillow_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/codeup-data-science/Zillow_Clustering_Project-/acquire.py\u001b[0m in \u001b[0;36mzillow_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# otherwise, pull the data from the database:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# establish database url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_db_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zillow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# establish query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     sql = '''\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "df = acquire.zillow_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f80b33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vb/2f5pphz943qdprdg5zvhqsbm0000gp/T/ipykernel_4672/3561961583.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_zillow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = prep_zillow(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91280ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, test_size=.2, validate_size=.3, random_state=42):\n",
    "    '''\n",
    "    This function takes in a dataframe, then splits that dataframe into three separate samples\n",
    "    called train, test, and validate, for use in machine learning modeling.\n",
    "\n",
    "    Three dataframes are returned in the following order: train, test, validate. \n",
    "    \n",
    "    The function also prints the size of each sample.\n",
    "    '''\n",
    "    # split the dataframe into train and test\n",
    "    train, test = train_test_split(df, test_size=.2, random_state=42)\n",
    "    # further split the train dataframe into train and validate\n",
    "    train, validate = train_test_split(train, test_size=.3, random_state=42)\n",
    "    # print the sample size of each resulting dataframe\n",
    "    print(f'train\\t n = {train.shape[0]}')\n",
    "    print(f'test\\t n = {test.shape[0]}')\n",
    "    print(f'validate n = {validate.shape[0]}')\n",
    "\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = train_validate_test_split(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0949e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'logerror'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(train, validate, test, k, col_list):\n",
    "    ''' \n",
    "    This function takes in a dataset split into three sample dataframes: train, validate and test.\n",
    "    It calculates an outlier range based on a given value for k, using the interquartile range \n",
    "    from the train sample. It then applies that outlier range to each of the three samples, removing\n",
    "    outliers from a given list of feature columns. The train, validate, and test dataframes \n",
    "    are returned, in that order. \n",
    "    '''\n",
    "    # Create a column that will label our rows as containing an outlier value or not\n",
    "    train['outlier'] = False\n",
    "    validate['outlier'] = False\n",
    "    test['outlier'] = False\n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q3 = train[col].quantile([.25, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # update the outlier label any time that the value is outside of boundaries\n",
    "        train['outlier'] = np.where(((train[col] < lower_bound) | (train[col] > upper_bound)) & (train.outlier == False), True, train.outlier)\n",
    "        validate['outlier'] = np.where(((validate[col] < lower_bound) | (validate[col] > upper_bound)) & (validate.outlier == False), True, validate.outlier)\n",
    "        test['outlier'] = np.where(((test[col] < lower_bound) | (test[col] > upper_bound)) & (test.outlier == False), True, test.outlier)\n",
    "\n",
    "    # remove observations with the outlier label in each of the three samples\n",
    "    train = train[train.outlier == False]\n",
    "    train = train.drop(columns=['outlier'])\n",
    "\n",
    "    validate = validate[validate.outlier == False]\n",
    "    validate = validate.drop(columns=['outlier'])\n",
    "\n",
    "    test = test[test.outlier == False]\n",
    "    test = test.drop(columns=['outlier'])\n",
    "\n",
    "    # print the remaining \n",
    "    print(f'train\\t n = {train.shape[0]}')\n",
    "    print(f'test\\t n = {test.shape[0]}')\n",
    "    print(f'validate n = {validate.shape[0]}')\n",
    "\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_columns = [col for col in df.columns if ((df[col].dtype != 'object') & (col not in [target, 'latitude', 'longitude']))]\n",
    "train, validate, test = remove_outliers(train, validate, test, 3, outlier_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729650dc",
   "metadata": {},
   "source": [
    "Note I think this removes entirely too much data (train from n=29001 to n=16969). We should probably find a more nuanced way to handle outliers (perhaps leave them in for some columns). But for now, we will drop them all and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc223a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885f168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e00c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc90c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1c652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa30c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af402d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531feb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ed619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0abe73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6aaefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eecfba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261de63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d0875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518c136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0822c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
